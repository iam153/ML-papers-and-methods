1、集成学习的概念
    按我自己的说法就是，原先我们学的一些算法比如svm，逻辑回归，感知机等等。我们原本是在一个问题中用一种算法去解决。但是单个不同的算法可能由于某种原因
  表现不佳，如果我们用多个算法训练之后组合起来预测效果会更好。这种方式叫集成学习。
    一般来说集成学习分为以下三大类：
      用于减少方差的bagging
      用于减少偏差的boosting
      用于提升预测结果的stacking
      
2、个体学习器的概念
    个体学习器就是第一问里面我们用来组合的算法实例。比如说我们用一些数据训练了一个svm，又用一些数据训练了一个感知机模型，那这2个模型都可以叫做个体学习器。
    
3、boosting bagging的概念、异同点
    bagging和boosting都是组合多个学习器的方法。不同的是bagging中各个学习器可以看做是“同步”训练的，即各个模型训练时使用各自从数据集中抽样得到的数据，训练互不影响，训练先后顺序不影响最终整合分类器的效果。但在boosting中后面的学习器训练依赖于前面学习器训练得到的某些信息（对某些样本的误判等信息）。
    
4、理解不同的结合策略(平均法，投票法，学习法)
    平均法：对多个分类器的预测结果的平均值（普通平均或者加权平均）作为整体的输出。个人感觉应该多用于回归预测类问题。
    投票法：若干个训练好的分类器对同一个问题作出判决，取判决结果数较多的结果作为整体的输出。如果分类器只见较为独立，则最终的结果很可能好于所有单个分类器。其实不难理解，假如分类器的准确率分别为p1,p2,p3...pn,那么投票的准确率就是1-(1-p1)*(1-p2)*(1-p3)*...*(1-pn)，这个式子是大于等于任何单个分类器的准确率的。
    学习法：将若干个学习器的输出结果作为输入，训练一个次级学习器（我感觉用逻辑回归就很合适）。这样的方法叫stacking，也是一种组合多个学习器的方法。
5、随机森林的思想
    有一堆决策树，然后我们让他们对一个样本投票，票数多的结果作为该样本的预测值。
6、随机森林的推广
    extra tree.
    totally random trees embedding.
    isolation tree.
7、随机森林的优缺点
    个人理解的优缺点如下：
    优点：实现简单、多棵树可以同时训练，可以通过计算oob误差得到森林构建过程中对于泛化误差的无偏估计。
    缺点；分类的时候有可能倾向于选择属性值较多的特征。
8、随机森林在sklearn中的参数解释
    这部分我看懂了之后直接挪过来了
    sklearn.ensemble.RandomForestClassifier

    class sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, max_depth=None,

    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’,

    max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True,

    oob_score=False, n_jobs=None, random_state=None, verbose=0,

    warm_start=False, class_weight=None)[source]

    n_estimators:森林中树的数量，初始越多越好，但是会增加训练时间，到达一定数量后模型的表现不会再有显著的提升

    criterion:特征选择的标准，有信息增益和基尼系数两种，使用信息增益的是ID3和C4.5算法（使用信息增益比），使用基尼系数的CART算法，默认是gini系数。

    max_depth:决策树最大深度，决策树模型先对所有数据集进行切分，再在子数据集上继续循环这个切分过程，max_depth可以理解成用来限制这个循环次数。

    min_samples_split:子数据集再切分需要的最小样本量，默认是2，如果子数据样本量小于2时，则不再进行下一步切分。如果数据量较小，使用默认值就可，如果数据量较大，为降低计算量，应该把这个值增大，即限制子数据集的切分次数。

    min_samples_leaf=1:叶节点（子数据集）最小样本数，如果子数据集中的样本数小于这个值，那么该叶节点和其兄弟节点都会被剪枝（去掉），该值默认为1。

    min_weight_fraction_leaf:在叶节点处的所有输入样本权重总和的最小加权分数，如果不输入则表示所有的叶节点的权重是一致的。

    max_features:特征切分时考虑的最大特征数量，默认是对所有特征进行切分，也可以传入int类型的值，表示具体的特征个数；也可以是浮点数，则表示特征个数的百分比；还可以是sqrt,表示总特征数的平方根；也可以是log2，表示总特征数的log个特征。

    max_leaf_nodes:最大叶节点个数，即数据集切分成子数据集的最大个数。

    min_impurity_decrease:切分点不纯度最小减少程度，如果某个结点的不纯度减少小于这个值，那么该切分点就会被移除。

    min_impurity_split:切分点最小不纯度，用来限制数据集的继续切分（决策树的生成），如果某个节点的不纯度（可以理解为分类错误率）小于这个阈值，那么该点的数据将不再进行切分。

    bootstrap=True:bootstrap采样，默认为True

    oob_score=False:oob（out of band，带外）数据，即：在某次决策树训练中没有被bootstrap选中的数据。多单个模型的参数训练，我们知道可以用cross validation（cv）来进行，但是特别消耗时间，而且对于随机森林这种情况也没有大的必要，所以就用这个数据对决策树模型进行验证，算是一个简单的交叉验证。性能消耗小，但是效果不错。

    n_jobs=None:n_jobs,并行化，可以在机器的多个核上并行的构造树以及计算预测值，不过受限于通信成本，可能效率并不会说分为k个线程就得到k倍的提升，不过整体而言相对需要构造大量的树或者构建一棵复杂的树而言还是高效的.

    random_state=None:随机数种子，类似于train_test_split分割时所使用的random_state

    verbose=0:是否显示任务进程。

    class_weight:权重设置，主要是用于处理不平衡样本，与LR模型中的参数一致，可以自定义类别权重，也可以直接使用balanced参数值进行不平衡样本处理。
9、随机森林的应用场景
    特征选择、分类、回归、异常检测等
